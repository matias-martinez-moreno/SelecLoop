# core/services/review_verification.py
import logging
import re

logger = logging.getLogger(__name__)

# Intentar importar dependencias de ML
try:
    import torch
    from transformers import pipeline
    ML_AVAILABLE = True
    logger.debug("ML libraries (torch, transformers) successfully imported")
except ImportError as e:
    ML_AVAILABLE = False
    logger.warning(f"torch or transformers not available: {e}. Will use basic checks only.")

class ReviewVerificationService:
    """
    Servicio Singleton para verificaci√≥n de rese√±as con modelos de Hugging Face.
    Los modelos se cargan una sola vez al crear la primera instancia y se reutilizan.
    """
    _instance = None
    _initialized = False
    
    def __new__(cls):
        """Patr√≥n Singleton: solo una instancia en toda la aplicaci√≥n"""
        if cls._instance is None:
            cls._instance = super(ReviewVerificationService, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        """Solo inicializar una vez, aunque se llame m√∫ltiples veces"""
        if not ReviewVerificationService._initialized:
            self.toxicity_pipeline = None
            self.sentiment_pipeline = None
            self.models_loaded = False
            self._load_models()  # ‚Üê Se carga SOLO la primera vez
            ReviewVerificationService._initialized = True
    
    def _load_models(self):
        if not ML_AVAILABLE:
            logger.info("‚ö†Ô∏è ML libraries not available, skipping model loading. Using basic checks only.")
            self.models_loaded = False
            return
            
        try:
            logger.info("üîÑ Loading Hugging Face models... This may take a few minutes on first run.")
            
            # Modelo para toxicidad y odio
            # Usar device=-1 para CPU (m√°s compatible) o 0 para GPU si est√° disponible
            try:
                self.toxicity_pipeline = pipeline(
                    "text-classification",
                    model="unitary/toxic-bert",
                    device=-1  # CPU para evitar problemas de GPU
                )
                logger.info("‚úÖ Toxicity model (unitary/toxic-bert) loaded successfully")
            except Exception as e:
                logger.error(f"‚ùå Error loading toxicity model: {e}")
                self.toxicity_pipeline = None
            
            # Modelo para sentimiento extremo
            try:
                self.sentiment_pipeline = pipeline(
                    "sentiment-analysis",
                    model="cardiffnlp/twitter-roberta-base-sentiment-latest",
                    device=-1  # CPU para evitar problemas de GPU
                )
                logger.info("‚úÖ Sentiment model (cardiffnlp/twitter-roberta-base-sentiment-latest) loaded successfully")
            except Exception as e:
                logger.error(f"‚ùå Error loading sentiment model: {e}")
                self.sentiment_pipeline = None
            
            # Verificar que al menos uno de los modelos se carg√≥
            if self.toxicity_pipeline is not None or self.sentiment_pipeline is not None:
                self.models_loaded = True
                logger.info("‚úÖ Hugging Face models loaded successfully. ML verification is now ACTIVE.")
            else:
                self.models_loaded = False
                logger.warning("‚ö†Ô∏è Failed to load any ML models. Using basic checks only.")
            
        except Exception as e:
            logger.error(f"Error loading models: {e}", exc_info=True)
            self.models_loaded = False
            logger.warning("Falling back to basic content checks only.")
    
    def verify_review(self, text):
        """
        Verifica si una rese√±a es apropiada - Anti-odio y anti-contenido fuera de lugar
        """
        # Log inicial muy visible - USAR WARNING para que se vea en consola
        logger.warning("=" * 80)
        logger.warning("üîç INICIANDO VERIFICACI√ìN DE RESE√ëA")
        logger.warning(f"üìù Texto a verificar (primeros 100 chars): {text[:100]}...")
        logger.warning(f"ü§ñ Modelos ML cargados: {self.models_loaded}")
        logger.warning(f"ü§ñ Toxicity pipeline disponible: {self.toxicity_pipeline is not None}")
        logger.warning(f"ü§ñ Sentiment pipeline disponible: {self.sentiment_pipeline is not None}")
        logger.warning("=" * 80)
        
        # Solo rechazar si es completamente vac√≠o
        if not text or len(text.strip()) < 1:
            logger.warning("‚ùå Texto vac√≠o - RECHAZADO")
            return {
                'is_appropriate': False,
                'reason': 'Texto vac√≠o',
                'confidence': 1.0,
                'category': 'insufficient_content'
            }
        
        try:
            # Verificaciones b√°sicas primero
            logger.warning("üîé Ejecutando verificaciones b√°sicas...")
            basic_check = self._comprehensive_content_check(text)
            if not basic_check['is_appropriate']:
                logger.warning("=" * 80)
                logger.warning(f"‚ùå‚ùå‚ùå RECHAZADO por verificaciones b√°sicas ‚ùå‚ùå‚ùå")
                logger.warning(f"   Raz√≥n: {basic_check['reason']}")
                logger.warning(f"   Confianza: {basic_check['confidence']}")
                logger.warning(f"   Categor√≠a: {basic_check['category']}")
                logger.warning("=" * 80)
                return basic_check
            logger.warning("‚úÖ‚úÖ‚úÖ Verificaciones b√°sicas PASADAS - Continuando con ML...")
            
            # Si los modelos no se cargaron, usar solo verificaciones b√°sicas
            if not self.models_loaded:
                logger.warning("=" * 80)
                logger.warning("‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è MODELOS ML NO CARGADOS - USANDO SOLO VERIFICACIONES B√ÅSICAS ‚ö†Ô∏è‚ö†Ô∏è‚ö†Ô∏è")
                logger.warning("=" * 80)
                return {
                    'is_appropriate': basic_check['is_appropriate'],
                    'reason': basic_check['reason'],
                    'confidence': basic_check['confidence'],
                    'category': basic_check['category']
                }
            
            # Log muy visible para confirmar que se est√° usando ML - USAR WARNING
            logger.warning("=" * 80)
            logger.warning("ü§ñü§ñü§ñ USANDO MODELOS ML PARA VERIFICACI√ìN ü§ñü§ñü§ñ")
            logger.warning(f"‚úÖ Toxicity pipeline: {self.toxicity_pipeline is not None}")
            logger.warning(f"‚úÖ Sentiment pipeline: {self.sentiment_pipeline is not None}")
            logger.warning("=" * 80)
            
            # Verificar toxicidad y odio con el modelo
            try:
                toxicity_score = 0
                toxic_categories = []
                sentiment_score = 0
                sentiment_label = 'NEUTRAL'
                
                # Verificar toxicidad si el modelo est√° disponible
                if self.toxicity_pipeline is not None:
                    try:
                        toxicity_result = self.toxicity_pipeline(text, top_k=None)  # Obtener todos los resultados
                        
                        # El modelo unitary/toxic-bert puede devolver diferentes formatos
                        # Puede ser una lista de diccionarios o un diccionario √∫nico
                        if toxicity_result:
                            # Normalizar a lista si es necesario
                            if not isinstance(toxicity_result, list):
                                toxicity_result = [toxicity_result]
                            
                            # Procesar todos los resultados
                            for result in toxicity_result:
                                if isinstance(result, dict):
                                    label = str(result.get('label', '')).upper()
                                    score = float(result.get('score', 0))
                                    
                                    # Categor√≠as t√≥xicas que queremos detectar
                                    toxic_labels = ['TOXIC', 'SEVERE_TOXIC', 'THREAT', 'INSULT', 'IDENTITY_ATTACK', 'OBSCENE', 'HATE']
                                    
                                    # Verificar si el label contiene alguna categor√≠a t√≥xica
                                    is_toxic = False
                                    for toxic_label in toxic_labels:
                                        if toxic_label in label:
                                            is_toxic = True
                                            if score > toxicity_score:
                                                toxicity_score = score
                                            if label not in toxic_categories:
                                                toxic_categories.append(label)
                                            break
                                    
                                    # Si no es una categor√≠a conocida pero el score es alto, tambi√©n considerarlo
                                    if not is_toxic and score > 0.6:
                                        toxicity_score = max(toxicity_score, score)
                                        if label not in toxic_categories:
                                            toxic_categories.append(label)
                                    
                                    # Tambi√©n considerar cualquier score alto como potencialmente t√≥xico
                                    if score > 0.5:
                                        toxicity_score = max(toxicity_score, score)
                                        
                    except Exception as tox_error:
                        logger.warning(f"Error in toxicity detection: {tox_error}", exc_info=True)
                        # Continuar con otros checks
                
                # Verificar sentimiento extremo si el modelo est√° disponible
                if self.sentiment_pipeline is not None:
                    try:
                        sentiment_result = self.sentiment_pipeline(text, top_k=None)  # Obtener todos los resultados
                        if sentiment_result and len(sentiment_result) > 0:
                            # El modelo puede devolver m√∫ltiples resultados, buscar el m√°s negativo
                            if isinstance(sentiment_result, list):
                                # Buscar el resultado m√°s negativo
                                for res in sentiment_result:
                                    if isinstance(res, dict):
                                        label = str(res.get('label', '')).upper()
                                        score = float(res.get('score', 0))
                                        # Si es negativo y tiene score alto, usarlo
                                        if any(neg in label for neg in ['NEGATIVE', 'NEG', 'LABEL_2', 'LABEL_1']) and score > sentiment_score:
                                            sentiment_score = score
                                            sentiment_label = label
                                        # Si no encontramos negativo, usar el primero
                                        elif sentiment_score == 0:
                                            sentiment_score = score
                                            sentiment_label = label
                            else:
                                sentiment_score = float(sentiment_result.get('score', 0))
                                sentiment_label = str(sentiment_result.get('label', 'NEUTRAL'))
                        
                        # Log para debugging - USAR WARNING
                        logger.warning(f"üìä ML Sentiment analysis - Score: {sentiment_score}, Label: {sentiment_label}")
                    except Exception as sent_error:
                        logger.warning(f"Error in sentiment analysis: {sent_error}")
                        # Continuar con otros checks
                
                # Log para debugging - USAR WARNING
                logger.warning(f"üìä ML Toxicity analysis - Score: {toxicity_score}, Categories: {toxic_categories}")
                
                # L√≥gica de decisi√≥n m√°s estricta
                is_appropriate = True
                reason = "Rese√±a apropiada"
                confidence = 0.5
                category = 'appropriate'
                
                # Normalizar etiqueta de sentimiento para comparaci√≥n
                sentiment_label_upper = str(sentiment_label).upper()
                is_negative_sentiment = any(neg in sentiment_label_upper for neg in ['NEGATIVE', 'NEG', 'LABEL_2', 'LABEL_1', 'LABEL_0'])
                
                # Detectar palabras clave muy negativas en el texto (fallback adicional)
                text_lower = text.lower()
                very_negative_keywords = ['fraude', 'estafan', 'incompetentes', 'desastre', 'terrible', 'horrible', 
                                        'desastrosa', 'fraudulento', 'estafa', 'mentirosos', 'mediocre', 'asombroso',
                                        'peor experiencia', 'completo desastre', 'p√©rdida de tiempo']
                has_very_negative_keywords = any(keyword in text_lower for keyword in very_negative_keywords)
                
                # Detectar odio y contenido ofensivo - UMBRALES M√ÅS AGRESIVOS
                # Prioridad 1: Toxicidad moderada/alta
                if toxicity_score > 0.4:  # Umbral m√°s bajo: 0.4
                    is_appropriate = False
                    reason = f"Contenido t√≥xico detectado: {', '.join(toxic_categories) if toxic_categories else 'contenido ofensivo'}"
                    confidence = min(toxicity_score, 0.99)
                    category = 'toxic'
                    logger.warning(f"Review rejected by ML - Toxicity: {toxicity_score}")
                # Prioridad 2: Sentimiento muy negativo (umbral m√°s bajo)
                elif sentiment_score > 0.55 and is_negative_sentiment:  # Umbral m√°s bajo: 0.55
                    is_appropriate = False
                    reason = "Sentimiento extremadamente negativo y agresivo detectado"
                    confidence = min(sentiment_score, 0.99)
                    category = 'hate_speech'
                    logger.warning(f"Review rejected by ML - Sentiment: {sentiment_score}, Label: {sentiment_label}")
                # Prioridad 3: Palabras clave muy negativas + sentimiento negativo
                elif has_very_negative_keywords and is_negative_sentiment and sentiment_score > 0.5:
                    is_appropriate = False
                    reason = "Contenido extremadamente negativo con lenguaje inapropiado detectado"
                    confidence = max(sentiment_score, 0.7)
                    category = 'hate_speech'
                    logger.warning(f"Review rejected by ML - Very negative keywords + sentiment: {sentiment_score}")
                # Prioridad 4: Toxicidad baja pero con sentimiento negativo fuerte
                elif toxicity_score > 0.25 and is_negative_sentiment and sentiment_score > 0.6:  # Umbral m√°s bajo
                    is_appropriate = False
                    reason = "Contenido potencialmente ofensivo detectado"
                    confidence = (toxicity_score + sentiment_score) / 2
                    category = 'toxic'
                    logger.warning(f"Review rejected by ML - Combined: toxicity={toxicity_score}, sentiment={sentiment_score}")
                # Prioridad 5: Solo sentimiento muy negativo (sin toxicidad)
                elif sentiment_score > 0.7 and is_negative_sentiment:
                    is_appropriate = False
                    reason = "Sentimiento extremadamente negativo detectado"
                    confidence = min(sentiment_score, 0.99)
                    category = 'hate_speech'
                    logger.warning(f"Review rejected by ML - Very negative sentiment only: {sentiment_score}")
                
                result = {
                    'is_appropriate': is_appropriate,
                    'reason': reason,
                    'confidence': confidence,
                    'category': category,
                    'toxicity_score': toxicity_score,
                    'sentiment_score': sentiment_score,
                    'sentiment_label': str(sentiment_label),
                    'toxic_categories': toxic_categories,
                    'ml_models_used': self.models_loaded
                }
                
                # Log final del resultado - MUY VISIBLE - USAR WARNING
                logger.warning("=" * 80)
                if is_appropriate:
                    logger.warning("‚úÖ‚úÖ‚úÖ ML VERIFICACI√ìN: APROBADA ‚úÖ‚úÖ‚úÖ")
                    logger.warning(f"   Raz√≥n: {reason}")
                    logger.warning(f"   Confianza: {confidence}")
                    logger.warning(f"   Toxicity Score: {toxicity_score}")
                    logger.warning(f"   Sentiment Score: {sentiment_score} ({sentiment_label})")
                else:
                    logger.warning("‚ùå‚ùå‚ùå ML VERIFICACI√ìN: RECHAZADA ‚ùå‚ùå‚ùå")
                    logger.warning(f"   Raz√≥n: {reason}")
                    logger.warning(f"   Confianza: {confidence}")
                    logger.warning(f"   Categor√≠a: {category}")
                    logger.warning(f"   Toxicity Score: {toxicity_score}")
                    logger.warning(f"   Sentiment Score: {sentiment_score} ({sentiment_label})")
                    logger.warning(f"   Toxic Categories: {toxic_categories}")
                logger.warning("=" * 80)
                
                return result
                
            except Exception as model_error:
                logger.error("=" * 80)
                logger.error("‚ùå‚ùå‚ùå ERROR AL USAR MODELOS ML ‚ùå‚ùå‚ùå")
                logger.error(f"   Error: {str(model_error)}")
                logger.error(f"   Traceback completo:")
                import traceback
                logger.error(traceback.format_exc())
                logger.error("   ‚ö†Ô∏è Usando fallback a verificaciones b√°sicas")
                logger.error("=" * 80)
                # Fallback a verificaciones b√°sicas
                return basic_check
            
        except Exception as e:
            logger.error("=" * 80)
            logger.error("‚ùå‚ùå‚ùå ERROR GENERAL EN VERIFICACI√ìN ‚ùå‚ùå‚ùå")
            logger.error(f"   Error: {str(e)}")
            logger.error(f"   Traceback completo:")
            import traceback
            logger.error(traceback.format_exc())
            logger.error("   ‚ö†Ô∏è Aprobando rese√±a por defecto para no bloquear contenido leg√≠timo")
            logger.error("=" * 80)
            # En caso de error, aprobar la rese√±a para no rechazar contenido leg√≠timo
            return {
                'is_appropriate': True,
                'reason': f'Error en verificaci√≥n autom√°tica, aprobada por defecto',
                'confidence': 0.5,
                'category': 'error'
            }
    
    def _comprehensive_content_check(self, text):
        """Verificaciones exhaustivas anti-odio y anti-contenido fuera de lugar"""
        
        # Palabras de odio y ofensivas en espa√±ol (expandida)
        hate_words = [
            'puta', 'mierda', 'joder', 'cabr√≥n', 'hijo de puta', 'est√∫pido', 
            'idiota', 'imb√©cil', 'gilipollas', 'maric√≥n', 'puto', 'hijueputa',
            'malparido', 'gonorrea', 'hijueputa', 'malparido', 'mamahuevo',
            'gonorrea', 'cerote', 'verga', 'pendejo', 'culero', 'chingar',
            'baboso', 'tarado', 'bobo', 'huev√≥n', 'marica', 'manco', 'in√∫til'
        ]
        
        # Palabras discriminatorias
        discriminatory_words = [
            'negro de mierda', 'india puta', 'chino marica', 'gordo asqueroso',
            'flaco desgraciado', 'feo del carajo', 'viejo mierda', 'mujer huevona',
            'hombre marica', 'gay puto', 'lesbiana asquerosa'
        ]
        
        # Contenido fuera de lugar - SPAM FINANCIERO (prioridad alta)
        spam_financial_keywords = [
            'bitcoin', 'forex', 'inversi√≥n', 'trading', 'crypto', 'criptomoneda',
            'invertir', 'ganar dinero', 'usd', 'd√≥lares', 'dollar', 'profits',
            'get rich', 'dinero f√°cil', 'sin riesgo', '100% seguro', 'ganancias'
        ]
        
        # Contenido fuera de lugar - OTROS TEMAS
        off_topic_indicators = [
            'medicina', 'doctor', 'hospital', 'enfermedad', 'tratamiento',
            'pol√≠tica', 'gobierno', 'presidente', 'elecciones', 'votar',
            'deportes', 'f√∫tbol', 'partido', 'equipo', 'jugador',
            'videojuegos', 'playstation', 'xbox', 'fifa', 'call of duty'
        ]
        
        text_lower = text.lower()
        
        # PRIORIDAD 1: Detectar SPAM financiero - SOLO 1 keyword es suficiente
        spam_count = 0
        for keyword in spam_financial_keywords:
            if keyword in text_lower:
                spam_count += 1
        
        if spam_count >= 1:  # Si tiene al menos 1 keyword de spam financiero
            return {
                'is_appropriate': False,
                'reason': 'Spam financiero o publicidad detectada',
                'confidence': 0.95,
                'category': 'spam'
            }
        
        # PRIORIDAD 2: Verificar palabras de odio
        hate_count = 0
        for word in hate_words:
            if word in text_lower:
                hate_count += 1
        
        if hate_count > 0:
            return {
                'is_appropriate': False,
                'reason': f'Lenguaje ofensivo detectado ({hate_count} palabras)',
                'confidence': 0.9,
                'category': 'hate_speech'
            }
        
        # PRIORIDAD 3: Verificar palabras discriminatorias
        discrimination_count = 0
        for phrase in discriminatory_words:
            if phrase in text_lower:
                discrimination_count += 1
        
        if discrimination_count > 0:
            return {
                'is_appropriate': False,
                'reason': 'Lenguaje discriminatorio y ofensivo detectado',
                'confidence': 0.95,
                'category': 'hate_speech'
            }
        
        # PRIORIDAD 4: Verificar contenido fuera de lugar (otros temas)
        off_topic_count = 0
        for indicator in off_topic_indicators:
            if indicator in text_lower:
                off_topic_count += 1
        
        if off_topic_count > 2:  # M√°s de 2 indicadores de contenido fuera de lugar
            return {
                'is_appropriate': False,
                'reason': 'Contenido fuera de lugar o no relacionado con la empresa',
                'confidence': 0.8,
                'category': 'off_topic'
            }
        
        # Verificar spam (repeticiones excesivas)
        words = text.split()
        if len(words) > 10:
            word_count = {}
            for word in words:
                if len(word) > 3:  # Solo palabras de m√°s de 3 caracteres
                    word_count[word] = word_count.get(word, 0) + 1
                    if word_count[word] > len(words) * 0.25:  # M√°s del 25% repetici√≥n
                        return {
                            'is_appropriate': False,
                            'reason': 'Posible spam detectado (repeticiones excesivas)',
                            'confidence': 0.8,
                            'category': 'spam'
                        }
        
        # NO rechazar por longitud - solo por contenido ofensivo o fuera de lugar
        # Si pasa todas las verificaciones anteriores, aprobar
        return {'is_appropriate': True, 'reason': 'Contenido apropiado', 'confidence': 0.5, 'category': 'appropriate'}
    
    def _detect_off_topic(self, text):
        """Detecta si el contenido est√° fuera de lugar"""
        off_topic_indicators = [
            'crypto', 'bitcoin', 'inversi√≥n', 'forex', 'trading',
            'medicina', 'doctor', 'hospital', 'enfermedad',
            'pol√≠tica', 'gobierno', 'presidente', 'elecciones',
            'deportes', 'f√∫tbol', 'partido', 'equipo'
        ]
        
        text_lower = text.lower()
        off_topic_count = sum(1 for indicator in off_topic_indicators if indicator in text_lower)
        
        return off_topic_count > 2
